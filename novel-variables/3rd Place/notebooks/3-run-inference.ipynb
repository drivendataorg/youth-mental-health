{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878ebe4-62e3-465f-929e-f84c1fcecbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb231f-0577-4ce8-b399-2f7b92b94e47",
   "metadata": {},
   "source": [
    "# Read Data and Model\n",
    "\n",
    "Competition data and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b77f2-7b58-4964-b7d4-e8a2b85ebedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrs = pd.read_csv('../data/raw/nvdrs-youth-restricted.csv')\n",
    "print('Number of Narratives',narrs.shape)\n",
    "\n",
    "iaa_narrs = pd.read_csv('../data/interim/annotations.csv')\n",
    "iaa_narrs = iaa_narrs[iaa_narrs.iaa!=1]\n",
    "iaa_narrs = iaa_narrs[~iaa_narrs['relevant'].isna()].drop_duplicates(['uid','Person'])\n",
    "print('Number of Annotations',iaa_narrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40963386-2174-4808-bb3a-80e93cac4610",
   "metadata": {},
   "source": [
    "Set parameters for GPU use\n",
    "\n",
    "_This is an NVIDIA RTX A6000 with 49140MiB of space; we use most of the GPU with the models we load_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82385f-9182-49ee-9712-880401dfa7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = '00000000:98:00.0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.chdir('/shared/0/projects/nvdrs')\n",
    "cache_dir = \"/shared/4/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2d179-e0da-4944-b17b-b35d556d52ba",
   "metadata": {},
   "source": [
    "Download model from huggingface (10 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109614b-44a9-40a3-a3c5-e2558df898d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token your_token_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df010b5-637e-4811-9f04-c9f44ad2605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#model_dir = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "#model_dir = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = LLM(model=model_dir, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218513b-252a-4368-841e-b7469fa0b434",
   "metadata": {},
   "source": [
    "Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811b951-1e9d-44e4-8715-e856484464be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code(code, agg):\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.crosstab(agg[code+'_llm']>0,agg[code]>0))\n",
    "    print()\n",
    "    print('Performance')\n",
    "    for score in [precision_score, recall_score, f1_score]:\n",
    "        print(score.__name__, score(agg[code]>0,agg[code+'_llm']>0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d8eef-3abf-461c-a4a3-3c12594dce43",
   "metadata": {},
   "source": [
    "# Turn Narratives into Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029fd4c-8273-4814-b5b8-8d2f2640762b",
   "metadata": {},
   "source": [
    "Llama prompt format: <https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3beaf77-6809-4667-8fc5-5bf159c31d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_template_zeroshot = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "I am a researcher studying suicide risk factors. You are a helpful AI question answering assistant, who answers all my questions.\n",
    "{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Narrative: {narr}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5063f13-d806-4d99-bd71-4ddb4c82bc55",
   "metadata": {},
   "source": [
    "Prompt to split narratives (paragraphs) into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee168c-3345-468a-b0d4-24a425cc8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_prompt = 'Split the following narrative into sentences. Format your output as a list of all sentences in the narrative.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ee669-1075-4f23-a5a4-2c3bb44c4639",
   "metadata": {},
   "source": [
    "Run inference on LE Narratives (9 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bec41c-24f3-434a-b825-69159ddc9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "le_sents = llm.generate([llama3_template_zeroshot.format(narr = x, prompt=sent_prompt) for x in narrs.NarrativeLE],\n",
    "                          sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "# Process outputs\n",
    "final_le_sents = []\n",
    "for output in le_sents:\n",
    "    blob = output.outputs[0].text\n",
    "    blob = [x for x in blob.split('\\n') if bool(re.search('^\\d{1,2}\\\\.',x))]\n",
    "    final_le_sents.append(blob)\n",
    "\n",
    "# Create DF with narrative UID and each sentence\n",
    "uids = []\n",
    "sents = []\n",
    "for i in range(len(final_le_sents)):\n",
    "    uid = narrs.uid[i]\n",
    "    for x in final_le_sents[i]:\n",
    "        uids.append(uid)\n",
    "        sents.append(x)\n",
    "le_sents_df = pd.DataFrame({'uid':uids, 'type':'LE', 'sentence':sents})\n",
    "\n",
    "le_sents_df.to_csv('../data/interim/competition-le-sentences.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c68df-2546-4ba1-865a-37e07a901178",
   "metadata": {},
   "source": [
    "Run inference on CME Narratives (8 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0a8b5-c1bd-487f-8ba1-d6ffe42debc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "cme_sents = llm.generate([llama3_template_zeroshot.format(narr = x, prompt=sent_prompt) for x in narrs.NarrativeCME],\n",
    "                          sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "# Process outputs\n",
    "final_cme_sents = []\n",
    "for output in cme_sents:\n",
    "    blob = output.outputs[0].text\n",
    "    blob = [x for x in blob.split('\\n') if bool(re.search('^\\d{1,2}\\\\.',x))]\n",
    "    final_cme_sents.append(blob)\n",
    "\n",
    "# Create DF with narrative UID and each sentence\n",
    "uids = []\n",
    "sents = []\n",
    "for i in range(len(final_cme_sents)):\n",
    "    uid = narrs.uid[i]\n",
    "    for x in final_cme_sents[i]:\n",
    "        uids.append(uid)\n",
    "        sents.append(x)\n",
    "cme_sents_df = pd.DataFrame({'uid':uids, 'type':'LE', 'sentence':sents})\n",
    "\n",
    "cme_sents_df.to_csv('../data/interim/competition-cme-sentences.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f849e80-71e7-4ca9-ac8d-43ecec357f12",
   "metadata": {},
   "source": [
    "Read prompts from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f561fab-3f3a-4eff-8eb2-92ec494c8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sents_df = pd.read_csv('../data/interim/competition-le-sentences.csv')\n",
    "cme_sents_df = pd.read_csv('../data/interim/competition-cme-sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15deeee7-3cca-49a6-b10a-c9fa4ffc9c51",
   "metadata": {},
   "source": [
    "# Relevance Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20c47c-cc7e-45c8-83e7-b1c47e05c998",
   "metadata": {},
   "source": [
    "Prompt template for sentences as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b74192-c116-4f28-8b0d-015202d4b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_template_zeroshot = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "I am a researcher studying suicide risk factors. You are a helpful AI question answering assistant, who answers all my questions.\n",
    "{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Sentence: {narr}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290362b-53fb-4266-8a94-67b3265cb832",
   "metadata": {},
   "source": [
    "Prompt to test whether a sentence mentions an online space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273b7c5-cc0b-4d37-804d-a20f2d612389",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_relevant = \"Does the following sentence talk about an online space? This includes social media, web searches, messaging, chat, email, viewing or posting content, online gaming, online schooling, or cyberbullying. This does not include texting. Answer Yes or No with no explanation.\"\n",
    "prompt_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098ee5c-3fd8-4e45-abc6-8a58f4921423",
   "metadata": {},
   "source": [
    "Examine outputs on one narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0f0c8-e35a-453f-95e9-7b19635f9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_uid = 'azpu'\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) \n",
    "                for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "le_relevant = llm.generate(prompts_list,\n",
    "                           sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "for i,x in enumerate(le_sents_df.sentence[le_sents_df.uid==target_uid]):\n",
    "    print(x)\n",
    "    print(le_relevant[i].outputs[0].text)\n",
    "\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) \n",
    "                for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "cme_relevant = llm.generate(prompts_list,\n",
    "                            sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "for i,x in enumerate(cme_sents_df.sentence[cme_sents_df.uid==target_uid]):\n",
    "    print(x)\n",
    "    print(cme_relevant[i].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0290964-5415-4793-ab99-6c22998e5958",
   "metadata": {},
   "source": [
    "Evaluate performance on test set (4 mins)\n",
    "\n",
    "_Note: Poor performance on the neutral and withdraw codes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b17b54-cc69-4b47-8b81-d63f4a7dac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = iaa_narrs.uid.tolist()\n",
    "annotators = iaa_narrs.Person.tolist()\n",
    "\n",
    "# Run inference on LE narratives\n",
    "le_annotators = [annotators[i] for i,target_uid in enumerate(ids) for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "le_uids = [target_uid for target_uid in ids for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "le_sents = [x for target_uid in ids for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) for x in le_sents]\n",
    "le_relevant = llm.generate(prompts_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "print(Counter([x.outputs[0].text for x in le_relevant]))\n",
    "\n",
    "# Run inference on CME narratives\n",
    "cme_annotators = [annotators[i] for i,target_uid in enumerate(ids) for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "cme_uids = [target_uid for target_uid in ids for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "cme_sents = [x for target_uid in ids for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) for x in cme_sents]\n",
    "cme_relevant = llm.generate(prompts_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "print(Counter([x.outputs[0].text for x in cme_relevant]))\n",
    "\n",
    "# Create combined dataset with both LE and CME sentences + labels\n",
    "sents_labeled = pd.concat([pd.DataFrame({'uid':le_uids, 'type':'LE', 'sentence':le_sents,\n",
    "                                         'person':le_annotators, \n",
    "                                         'relevant_llm':[x.outputs[0].text for x in le_relevant]}),\n",
    "                            pd.DataFrame({'uid':cme_uids, 'type':'CME', 'sentence':cme_sents, \n",
    "                                          'person':cme_annotators, \n",
    "                                          'relevant_llm':[x.outputs[0].text for x in cme_relevant]})])\n",
    "sents_labeled['relevant_llm'] = sents_labeled['relevant_llm'].apply(lambda x: 1 if x=='Yes' else 0)\n",
    "sents_labeled = sents_labeled.reset_index(drop=True)\n",
    "\n",
    "# Assign labels to each narrative (relevant_llm = 1 if any sentence was classified as relevant)\n",
    "agg = sents_labeled.groupby(['uid','person'])['relevant_llm'].sum().reset_index().merge(iaa_narrs[['uid','relevant']],on='uid',how='left')\n",
    "agg2 = sents_labeled.groupby(['uid','person'])['relevant_llm'].sum().reset_index().merge(iaa_narrs,on='uid',how='left')\n",
    "\n",
    "# Evaluate performance\n",
    "evaluate_code('relevant', agg)\n",
    "for c in ['disclosure', 'disclosure_violent', 'sharing', 'conflict',\n",
    "       'withdraw', 'harm_passive', 'harm_active', 'victim', 'neutral',\n",
    "       'time_intensive', 'relationship', 'le_search']:\n",
    "    print(pd.crosstab(agg2[c],agg2['relevant_llm']>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9b6a7-25c9-4794-a153-fc8eae950ba5",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences (25 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83e2e2-ed3b-4b9f-884b-3a6a8f11ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = narrs.uid.tolist()\n",
    "\n",
    "le_uids = [target_uid for target_uid in ids for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "le_sents = [x for target_uid in ids for x in le_sents_df.sentence[le_sents_df.uid==target_uid]]\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) for x in le_sents]\n",
    "le_relevant = llm.generate(prompts_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "print(Counter([x.outputs[0].text for x in le_relevant]))\n",
    "\n",
    "cme_uids = [target_uid for target_uid in ids for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "cme_sents = [x for target_uid in ids for x in cme_sents_df.sentence[cme_sents_df.uid==target_uid]]\n",
    "prompts_list = [llama3_template_zeroshot.format(narr = x, prompt=prompt_relevant) for x in cme_sents]\n",
    "cme_relevant = llm.generate(prompts_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "print(Counter([x.outputs[0].text for x in cme_relevant]))\n",
    "\n",
    "sents_labeled_final = pd.concat([pd.DataFrame({'uid':le_uids, 'type':'LE', 'sentence':le_sents,\n",
    "                                               'relevant_llm':[x.outputs[0].text for x in le_relevant]}),\n",
    "                                 pd.DataFrame({'uid':cme_uids, 'type':'CME', 'sentence':cme_sents,\n",
    "                                               'relevant_llm':[x.outputs[0].text for x in cme_relevant]})])\n",
    "sents_labeled_final['relevant_llm'] = sents_labeled_final['relevant_llm'].apply(lambda x: 1 if x=='Yes' else 0)\n",
    "sents_labeled_final = sents_labeled_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48327a58-46c4-4cf7-a51a-92dac0db936b",
   "metadata": {},
   "source": [
    "# Disclosure Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6936da-5240-4a59-a0bb-5910f54cb5b0",
   "metadata": {},
   "source": [
    "Prompt to see whether a sentence discusses suicidal ideation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c3155-8d65-4931-90b5-ce4fb4fb4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_disclosure = 'In the following sentence, was V thinking about suicide, planning to kill or hurt themselves, or indicating suicidal ideation? Answer Yes or No with no explanation.'\n",
    "prompt_disclosure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b672794-b681-41ec-a387-3fb3f61af312",
   "metadata": {},
   "source": [
    "Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cddc5-705f-49ba-a081-514cbacfffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_disclosure) \n",
    "               for x in sents_labeled[sents_labeled.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'disclosure'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[sents_labeled.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x=='Yes' else 0)\n",
    "agg = sents_labeled.groupby(['uid','person'])[code+'_llm'].sum().reset_index().\\\n",
    "        merge(iaa_narrs[['uid',code,'disclosure_violent']],on='uid',how='left')\n",
    "agg['disclosure'] = agg['disclosure'] + agg['disclosure_violent']\n",
    "evaluate_code(code,agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57194d60-9468-4de3-a3bb-d49d02ab8ca1",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences that mention online spaces (15 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff9a90-89a7-4aba-b38f-6fe2b687775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_disclosure) \n",
    "               for x in sents_labeled_final[sents_labeled_final.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'disclosure'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[sents_labeled_final.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x=='Yes' else 0)\n",
    "Counter(sents_labeled[code+'_llm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c3da6-d7ca-40d8-a34c-972f38c6d4ae",
   "metadata": {},
   "source": [
    "# Sharing Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641afc42-4a10-4056-999c-24b4b4839104",
   "metadata": {},
   "source": [
    "Prompt to see whether a sentence discusses sharing intimate information or emotions online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89ba7a-2649-47aa-a8c9-7c44eb3e87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sharing = \"\"\"In the following sentence, which of the following is true? Give only the letter with no explanation.\n",
    "\n",
    "A. V posted on social media or messaged someone indicating they were thinking about suicide or planning to kill or hurt themselves\n",
    "B. V searched how to kill or hurt themselves online\n",
    "C. V had an interpersonal issue (argument, breakup, conflict, etc.)\n",
    "D. V left a suicide note \n",
    "E. V messaged someone online\n",
    "F. V talked about non-suicidal self-harm online\n",
    "G. V posted about their thoughts or feelings online.\n",
    "H. V revealed something about themselves online\n",
    "I. V posted something private or personal online\n",
    "J. Someone reported V's suicidal ideation online\n",
    "K. None of the above.\"\"\"\n",
    "print(prompt_sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f3843-d776-442b-b64a-3241d202ebec",
   "metadata": {},
   "source": [
    "Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee70df-37da-4076-8cf1-92ac896b83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_sharing) \n",
    "               for x in sents_labeled[sents_labeled.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'sharing'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[sents_labeled.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x in ['F','G','H','I'] else 0)\n",
    "agg = sents_labeled.groupby(['uid','person'])[['disclosure_llm',code+'_llm']].sum().reset_index().\\\n",
    "        merge(iaa_narrs[['uid',code]],on='uid',how='left')\n",
    "evaluate_code(code,agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a18ad-e335-4fc2-94f4-8562bd4ec9cd",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences that mention online spaces (30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b919e1-06ce-4e6b-b6de-2e1654578ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_sharing) \n",
    "               for x in sents_labeled_final[sents_labeled_final.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'sharing'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[sents_labeled_final.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x in ['F','G','H','I'] else 0)\n",
    "Counter(sents_labeled[code+'_llm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e56efb-8b6d-4402-a560-386bb94f6fbb",
   "metadata": {},
   "source": [
    "# Conflict Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987df67-9122-44b9-b1e0-991f445bcec1",
   "metadata": {},
   "source": [
    "Prompt to see whether a sentence discusses a conflict that started or progressed online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b615e6-cb61-44a6-b5ac-9784c0a2b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_conflict = \"\"\"In the following sentence, which of the following is true? Give only the letter with no explanation.\n",
    "\n",
    "A. V posted on social media or messaged someone indicating they were thinking about suicide or planning to kill or hurt themselves\n",
    "B. V was being bullied or harassed online. \n",
    "C. V argued with somene online.\n",
    "D. Something happened online which led to a conflict.\n",
    "E. V posted about an interpersonal conflict online. \n",
    "F. An online relationship was in trouble or ended. \n",
    "G. None of the above.\"\"\"\n",
    "print(prompt_conflict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740d248-6974-42db-8937-4d77b4c09b23",
   "metadata": {},
   "source": [
    "Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a893155-b2a7-471c-850a-300ff7cc5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_conflict) \n",
    "               for x in sents_labeled[sents_labeled.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'conflict'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[sents_labeled.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x in ['C','D','E','F'] else 0)\n",
    "agg = sents_labeled.groupby(['uid','person'])[['disclosure_llm',code+'_llm']].sum().reset_index().\\\n",
    "        merge(iaa_narrs[['uid',code]],on='uid',how='left')\n",
    "evaluate_code(code,agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bef99-60b2-4e79-bd6b-34802c7c9961",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences that mention online spaces (30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80883a2-e96c-46db-ad0d-bda8af39402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_conflict) \n",
    "               for x in sents_labeled_final[sents_labeled_final.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'conflict'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[sents_labeled_final.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x in ['C','D','E','F'] else 0)\n",
    "Counter(sents_labeled[code+'_llm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56269ecc-4930-4180-8b85-ca0bbe8d0a8f",
   "metadata": {},
   "source": [
    "# Withdraw Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6339d-14d2-4cbb-a514-fe8e8bf0f76d",
   "metadata": {},
   "source": [
    "Prompt to see whether a sentence discusses withdrawal from online spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68fedb-1cc3-45e9-9f4e-05b0b2c80ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_withdraw = \"\"\"In the following sentence, which of the following is true? Give only the letter with no explanation.\n",
    "\n",
    "A. V posted on social media or messaged someone indicating they were thinking about suicide or planning to kill or hurt themselves\n",
    "B. Someone took away V's access to internet, phone, computer, gaming, social media, or other devices\n",
    "C. V had stopped using social media, deleted an account, or withdrew from an online account\n",
    "D. None of the above\"\"\"\n",
    "print(prompt_withdraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec5547-061e-4f0b-ac61-e7a45db64586",
   "metadata": {},
   "source": [
    "Evaluate performance on test set (all narratives, not just those that mention online spaces) (4 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366d25c-92ef-4068-aec5-594ae589f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_withdraw) \n",
    "               for x in sents_labeled.sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'withdraw'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[:,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x in ['B','C'] else 0)\n",
    "\n",
    "# Calculate interannotator agreement at a narrative level \n",
    "agg = sents_labeled.groupby(['uid','person'])[['disclosure_llm',code+'_llm']].max().reset_index().\\\n",
    "    merge(iaa_narrs[['uid',code]],on='uid',how='left')\n",
    "evaluate_code(code,agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8616c-6d39-4499-bdc0-3c5cd497730e",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences (not just those that mention online spaces) (34 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0bd47-515c-4850-a4b1-c45ba1dc641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_withdraw) \n",
    "               for x in sents_labeled_final.sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'withdraw'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[:,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x in ['B','C'] else 0)\n",
    "Counter(sents_labeled[code+'_llm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a54a4c-142a-4a3f-857b-bb92d4ea4945",
   "metadata": {},
   "source": [
    "# Harm / Victimization Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820aa3f-6dd3-489e-b2b6-8234c9d9642a",
   "metadata": {},
   "source": [
    "Prompt to see whether a sentence discusses harm by or to the decedent in online spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60173480-3076-4c00-b1bb-638274c97b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_harm = \"\"\"In the following sentence, which of the following is true? Give only the letter with no explanation.\n",
    "\n",
    "A. V posted on social media or messaged someone indicating they were thinking about suicide or planning to kill or hurt themselves\n",
    "B. V searched how to kill or hurt themselves online\n",
    "C. V argued with someone\n",
    "D. V talked about non-suicidal self-harm online\n",
    "E. V was bullied, harassed, or harmed online\n",
    "F. V harmed, threatened, acted inappropriately towards, or bullied someone online\n",
    "G. V was on a forum for suicide or self harm\n",
    "H. V viewed other violent or explicit content online\n",
    "I. None of the above.\"\"\"\n",
    "print(prompt_harm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea52b3f-f9a7-4db8-b1e2-1c2cdc05afce",
   "metadata": {},
   "source": [
    "Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec683d5f-8387-47dc-8863-01cf11bb19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_harm) \n",
    "               for x in sents_labeled[sents_labeled.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'harm_passive'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[sents_labeled.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x in ['B','G','H'] else 0)\n",
    "agg = sents_labeled.groupby(['uid','person'])[['disclosure_llm',code+'_llm']].sum().reset_index().\\\n",
    "        merge(iaa_narrs[['uid',code]],on='uid',how='left')\n",
    "print(code)\n",
    "evaluate_code(code,agg)\n",
    "\n",
    "\n",
    "code = 'victim'\n",
    "sents_labeled[code+'_llm'] = 'n'\n",
    "sents_labeled.loc[sents_labeled.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled[code+'_llm'] = sents_labeled[code+'_llm'].apply(lambda x: 1 if x in ['E'] else 0)\n",
    "agg = sents_labeled.groupby(['uid','person'])[['disclosure_llm',code+'_llm']].sum().reset_index().\\\n",
    "        merge(iaa_narrs[['uid',code]],on='uid',how='left')\n",
    "print(code)\n",
    "evaluate_code(code,agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bceca-5937-489c-9dc1-955f16faca89",
   "metadata": {},
   "source": [
    "Generate predictions on all sentences that mention online spaces (30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb2a37-664d-44e9-9a72-e6cce33c9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [llama3_template_zeroshot.format(narr = x, prompt = prompt_harm) \n",
    "               for x in sents_labeled_final[sents_labeled_final.relevant_llm==1].sentence]\n",
    "codes = llm.generate(prompt_list, sampling_params=SamplingParams(max_tokens=8192, temperature=0))\n",
    "\n",
    "code = 'harm_passive'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[sents_labeled_final.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x in ['B','G','H'] else 0)\n",
    "print(code, Counter(sents_labeled[code+'_llm']))\n",
    "\n",
    "\n",
    "code = 'victim'\n",
    "sents_labeled_final[code+'_llm'] = 'n'\n",
    "sents_labeled_final.loc[sents_labeled_final.relevant_llm==1,code+'_llm'] = [x.outputs[0].text for x in codes]\n",
    "sents_labeled_final[code+'_llm'] = sents_labeled_final[code+'_llm'].apply(lambda x: 1 if x in ['E'] else 0)\n",
    "print(code, Counter(sents_labeled[code+'_llm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf960bd-9ef6-4bdb-b5de-d70d730ae77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ffdc05-77fc-44bd-9a75-1e9146b8677f",
   "metadata": {},
   "source": [
    "# Assemble Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986608ac-81fb-4c10-bf1b-0fa14e4c7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrs_labeled = sents_labeled_final.groupby(['uid'])[['relevant_llm', 'disclosure_llm',\n",
    "                                                      'sharing_llm', 'conflict_llm', 'withdraw_llm',\n",
    "                                                      'harm_passive_llm', 'victim_llm', ]].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f6402-0b8c-4b0b-b289-c2cb83b20cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['relevant','disclosure', 'sharing','conflict','withdraw', 'harm_passive','victim']:\n",
    "    print(var)\n",
    "    print(Counter(narrs_labeled[var+'_llm']))\n",
    "    print(np.mean(narrs_labeled[var+'_llm']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cd322-03e8-4987-80db-4c30583433ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrs_labeled.to_csv('../data/processed/competition_predictions.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
