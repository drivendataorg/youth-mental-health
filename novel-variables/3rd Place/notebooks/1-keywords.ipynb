{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction and Analysis (<1 min)\n",
    "This notebook reads in the provided sample data, tokenizes the narratives, identifies possible keywords using log odds, and analyzes the final set of chosen keywords. The analysis includes calculating the total number of narratives containing each keyword and organizes keywords by topic for Table 2 in the final submission. \n",
    "\n",
    "Create a virtual environment using `requirements.txt`, which should provide all necessary packages for this notebook (and notebook 2, but not notebook 3).\n",
    "\n",
    "#### Imports and Reading Data\n",
    "Creates `cleaned-nvdrs-youth-restricted.csv`, which just adds a column for the combined LE and CME narratives. It may be the case that the excluded NLTK downloads are required on the first run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#! might have to run these lines once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "narratives = pd.read_csv(\"../data/raw/nvdrs-youth-restricted.csv\")\n",
    "narratives[\"combined_narratives\"] = narratives[\"NarrativeLE\"] + \\\n",
    "    narratives['NarrativeCME']\n",
    "narratives.to_csv(\"../data/interim/cleaned-nvdrs-youth-restricted.csv\", index=False)\n",
    "narratives = narratives[[\n",
    "    \"uid\", \"combined_narratives\", \"DisclosedToSocialMedia\"]]\n",
    "narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "Creates `narr-tokens.csv`, a CSV file containing the tokenized text of the narrative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "narrative_tokens = list()\n",
    "\n",
    "# Token filter; removes stop words, punctuation, small words, and all 'X' words\n",
    "def token_filter(word) : \n",
    "  return (word not in stop_words) & (word not in string.punctuation) & (len(word) > 2) & any(char != 'x' for char in word) and not any(char.isdigit() for char in word)\n",
    "\n",
    "for narrative in narratives['combined_narratives'] : \n",
    "  # Handle punctuation between words without spaces (e.g. 'vehicle.the')\n",
    "  narrative = re.sub(r'([a-zA-Z])\\.([a-zA-Z])', r'\\1 \\2', narrative)\n",
    "  narrative = re.sub(r'([a-zA-Z]),([a-zA-Z])', r'\\1 \\2', narrative)\n",
    "\n",
    "  tokens = [word.lower() for word in word_tokenize(narrative) if token_filter(word.lower())]\n",
    "  narrative_tokens.append(tokens)\n",
    "\n",
    "narrative_tokens = pd.DataFrame(narrative_tokens)\n",
    "narrative_tokens['DisclosedToSocialMedia'] = narratives['DisclosedToSocialMedia']\n",
    "narrative_tokens.to_csv('../data/interim/narr-tokens.csv', index=False)\n",
    "narrative_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Vector\n",
    "Creates `potential-kws.csv`, a list of words similar to the source words which should be considered to be used as a keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_tokens = pd.read_csv('../data/interim/narr-tokens.csv', low_memory=False)\n",
    "narrative_tokens = narrative_tokens.loc[:, ~narrative_tokens.columns.str.contains('^Unnamed')]\n",
    "narrative_tokens = narrative_tokens.apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "narrative_tokens = narrative_tokens.to_list()\n",
    "\n",
    "model1 = gensim.models.Word2Vec(narrative_tokens, min_count=10,\n",
    "                                vector_size=100, window=5)\n",
    "\n",
    "source_words = ['image', 'posting', 'copy', 'monitor', 'posted', 'reply', 'instagram', 'facebook']\n",
    "\n",
    "similar_words = set()\n",
    "\n",
    "for word in source_words: \n",
    "  for similar_word in model1.wv.most_similar(word, topn=25):\n",
    "    similar_words.add(similar_word[0])\n",
    "\n",
    "similar_words = pd.DataFrame(similar_words)\n",
    "similar_words.to_csv('../data/interim/potential-kws.csv', index=None)\n",
    "similar_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword Extraction\n",
    "Creates `tokens.csv`, a list of words from all narratives with their respective log odds which is used to identify keyword candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RARITY_THRESHOLD = 12\n",
    "\n",
    "narrative_tokens = pd.read_csv(\"../data/interim/narr-tokens.csv\", low_memory=False)\n",
    "D_mask = narrative_tokens['DisclosedToSocialMedia']\n",
    "D_mask = D_mask.apply(lambda x: x == 1)\n",
    "narrative_tokens = narrative_tokens.drop('DisclosedToSocialMedia', axis=1)\n",
    "\n",
    "# count number of occurences of each token in a series of narratives filtered by mask\n",
    "def count_tokens(narrs, mask):\n",
    "  narrs = narrs[mask].apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "  narrs = narrs.apply(lambda x: [i for i in x if i != ''])\n",
    "  narrs = narrs.apply(lambda x: Counter(x)).tolist()\n",
    "  narrs = pd.DataFrame(narrs).fillna(0).astype(int)\n",
    "  tokens = narrs.sum(axis=0).to_frame('count')\n",
    "  return tokens\n",
    "\n",
    "D_tokens = count_tokens(narrative_tokens, D_mask)\n",
    "ND_tokens = count_tokens(narrative_tokens, ~D_mask)\n",
    "\n",
    "# Merge tokens\n",
    "tokens = pd.merge(D_tokens, ND_tokens, how='outer', left_index=True, right_index=True, suffixes=('_D', '_ND'))\n",
    "tokens = tokens.fillna(0)\n",
    "tokens['count_D'] = tokens['count_D'] + 1\n",
    "tokens['count_ND'] = tokens['count_ND'] + 1\n",
    "\n",
    "# Compute total count across all disclosure/non-disclosure narratives\n",
    "total_D_tokens = tokens['count_D'].sum()\n",
    "print('Total Disclosure Tokens:', total_D_tokens)\n",
    "total_ND_tokens = tokens['count_ND'].sum()\n",
    "print('Total Non-Disclosure Tokens:', total_ND_tokens)\n",
    "\n",
    "# Features\n",
    "tokens['count'] = tokens['count_D'] + tokens['count_ND']\n",
    "tokens = tokens[tokens['count'] > RARITY_THRESHOLD]\n",
    "tokens['prob_D'] = tokens['count_D'] / total_D_tokens\n",
    "tokens['prob_ND'] = tokens['count_ND'] / total_ND_tokens\n",
    "tokens['odds_D'] = tokens['prob_D'] / (1 - tokens['prob_D'])\n",
    "tokens['odds_ND'] = tokens['prob_ND'] / (1 - tokens['prob_ND'])\n",
    "tokens['odds_ratio'] = tokens['odds_D'] / tokens['odds_ND']\n",
    "tokens['log_odds'] = tokens['odds_ratio'].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "\n",
    "# Cleanup\n",
    "tokens = tokens.sort_values(['log_odds'], ascending=[False])\n",
    "tokens.index.name = 'word'\n",
    "tokens.sort_values(['count'], ascending=[False], inplace=True)\n",
    "tokens.to_csv(\"../data/interim/tokens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword Analysis\n",
    "Creates `topic-crosstab.csv`, a table with the counts and prevalence values of each group of keywords in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeout_text(text):\n",
    "  text = re.search('message', re.sub('text messag', '', text))\n",
    "  return text\n",
    "\n",
    "\n",
    "narratives['tokens'] = narrative_tokens.apply(\n",
    "  lambda row: row.dropna().tolist(), axis=1)\n",
    "narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives['found_keywords'] = ''\n",
    "narratives['topics'] = [[] for _ in range(len(narratives))]\n",
    "narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(row, keyword):\n",
    "  if ' ' in keyword:\n",
    "    keyword_parts = keyword.split()\n",
    "    return any(row['tokens'][i:i + len(keyword_parts)] == keyword_parts for i in range(len(row['tokens']) - len(keyword_parts) + 1))\n",
    "  else:\n",
    "    return any(word == keyword for word in row['tokens'])\n",
    "\n",
    "\n",
    "topics = {\n",
    "  'Web': ['online', 'cyber', 'account', 'web', \"website\", \"webpage\", \"webpages\"],\n",
    "  'Social Media': ['facebook', 'instagram', 'youtube', 'twitter', 'discord', 'network', 'forum', 'app', 'apps', 'delete', 'deleted', \"caption\"],\n",
    "  'Message': ['post', 'posts', 'posted', 'posting', 'chat', 'chatting', 'chatted', 'chatroom', 'chatrooms', \"message\", \"messaged\", \"messages\", \"messaging\"],\n",
    "  'Other': ['game', 'gaming', 'games', 'dating', 'porn', 'stalk', 'image', 'stream', 'computer', 'laptop', 'internet', 'email', 'emailed', 'search history', 'search engine', 'device', \"electronic\", \"digital\", 'browser']\n",
    "}\n",
    "\n",
    "keywords = pd.DataFrame([(keyword, topic) for topic, keywords in topics.items(\n",
    ") for keyword in keywords], columns=['keyword', 'topic'])\n",
    "keywords.groupby('topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in narratives.iterrows():\n",
    "  found_keywords = []\n",
    "  for keyword in keywords['keyword']:\n",
    "    if create_mask(row, keyword):\n",
    "      found_keywords.append(keyword)\n",
    "\n",
    "  narratives.at[idx, 'found_keywords'] = ', '.join(found_keywords)\n",
    "  narratives.at[idx, 'topics'] = list(set(\n",
    "    [keywords.loc[keywords['keyword'] == keyword, 'topic'].iloc[0] for keyword in found_keywords]))\n",
    "\n",
    "narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.DataFrame(columns=[\n",
    "    'Keyword', 'Total', 'Social Media Disclosure', 'No Social Media Disclosure', 'Topic'])\n",
    "\n",
    "for index, keyword in enumerate(keywords['keyword']):\n",
    "  contains_KW = narratives['tokens'].apply(\n",
    "    lambda x: create_mask({'tokens': x}, keyword))\n",
    "\n",
    "  filtered_youth_narratives = narratives[contains_KW]\n",
    "\n",
    "  crosstab_values = filtered_youth_narratives['DisclosedToSocialMedia'].value_counts().to_dict()\n",
    "\n",
    "  crosstab = pd.concat([crosstab, pd.DataFrame({\n",
    "      'Keyword': [keyword], \n",
    "      'Total': [contains_KW.sum()], \n",
    "      'Social Media Disclosure': [crosstab_values.get(True, 0)], \n",
    "      'No Social Media Disclosure': [crosstab_values.get(False, 0)], \n",
    "      'Topic': [keywords.loc[index, 'topic']]\n",
    "  })], ignore_index=True)\n",
    "\n",
    "crosstab = crosstab.sort_values(by=['Topic', 'Total'], ascending=[True, False])\n",
    "crosstab.to_csv('../data/interim/used-keyword-analysis.csv', index=False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_crosstab_values = pd.DataFrame(columns=['Topic', 'Total', 'Social Media Disclosure', 'No Social Media Disclosure'])\n",
    "\n",
    "for topic in narratives['topics'].explode().unique():\n",
    "    filtered_youth_narratives = narratives[narratives['topics'].apply(lambda x: topic in x)]\n",
    "\n",
    "    crosstab_addition = pd.crosstab(index=filtered_youth_narratives['DisclosedToSocialMedia'],\n",
    "                           columns=['Total']).transpose()\n",
    "\n",
    "    crosstab_columns = crosstab_addition.columns.tolist()\n",
    "    crosstab_values = {\n",
    "        'Topic': topic,\n",
    "        'Total': filtered_youth_narratives.shape[0],\n",
    "        'Social Media Disclosure': crosstab_addition[1].iloc[0] if 1 in crosstab_columns else 0,\n",
    "        'No Social Media Disclosure': crosstab_addition[0].iloc[0] if 0 in crosstab_columns else 0\n",
    "    }\n",
    "\n",
    "    topic_crosstab_values = pd.concat([topic_crosstab_values, pd.DataFrame([crosstab_values])], ignore_index=True)\n",
    "\n",
    "topic_crosstab_values.to_csv('../data/processed/topic-crosstab.csv', index=False)\n",
    "topic_crosstab_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = narratives[narratives['found_keywords'] != '']\n",
    "\n",
    "narratives_count = narratives.shape[0]\n",
    "disclosure_count = narratives['DisclosedToSocialMedia'].sum()\n",
    "non_disclosure_count = narratives.shape[0] - disclosure_count\n",
    "\n",
    "print(f\"Total: {narratives_count}\")\n",
    "print(f\"Disclosure Count: {disclosure_count}\")\n",
    "print(f\"Non-Disclosure Count: {non_disclosure_count}\")\n",
    "\n",
    "subset = narratives[['uid', 'combined_narratives', 'found_keywords', \"DisclosedToSocialMedia\"]]\n",
    "subset = subset.sample(frac=1).reset_index(drop=True)\n",
    "subset.to_csv(\"../data/interim/subset.csv\", index=False)\n",
    "subset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
