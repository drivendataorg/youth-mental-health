# Solution -  Gemma2 models trained with LLM-generated data in a semi-supervised way

Username: dylanliu

## Summary

This solution trains Gemma2 models with repeated semi-supervised learning on the very small official dataset with only 4000 examples.

# Setup

1. Install Python>=3.10.14 and Pytroch>=2.1.0+CUDA if you don't have them on your machine.
How to install Python: https://docs.python.org/3/using/unix.html#on-linux  
How to install Pytroch: https://pytorch.org/get-started/previous-versions/  

2. Install the required Python packages:
`pip install -r requirements.txt`

3. Download gemma2 base models:  
gemma2-2b: https://huggingface.co/google/gemma-2-2b-it
gemma2-9b: https://huggingface.co/google/gemma-2-9b-it  

Rename model folders to 'gemma2_2b_it' and 'gemma2_9b_it', and move them to 'assets/'  

You can also use the base models from the submission codebase, where gemma2-9b is 8bit bitsandbytes quantizated for inference on one T4 GPU.  
The training code automatically processes bitsandbytes quantizated models. If you are using only one 4090 or another same sized GPU, 8bit quantizated gemma2-9b should fixs well, but bfloat16 training is recommended to get a better score if you have more GPU resource (The training code also automatically uses all available GPUs).  


# Run training

Then run training by the command line:   
`python train.py`  
After training, the trained models can be found under the code folder.  

You can also split the training by modifying 'config.py'.  


# Run inference

Move all the models to 'assets/', copy test features to 'data/test_features.csv' or modifying 'DATA_PATH' in 'config.py', then run inference by the command line: 
`python test.py`   
After inference, you can find the prediction file 'submission.csv' under the code folder.  


# Generated data

'data/10202_valpreds.csv': This is the predicted labels generated by the old deberta-v3-large model. This data is used for data label correction.  
'data/11071_padright_p1_alldata_to_1106.csv' and 'data/11071_padright_p1_alldata_to_1106.csv': These are the data generated by multiple semi-supervised learning processes, including the official data with soft labels. The generated data was generated using 'supplement/unlabeled_data_generation.py'.  
'data/data_with_preds10151_10152.csv`: The data was generated using 'supplement/data_generation.py'.  

# Supplement code  

'supplement/bugged_deberta.py': This is the code example of the old deberta-v3-large model. It's bugged that cv is only 0.82, but was used for data label correction. Use of this code is not recommended.   
'supplement/data_generation': This is a data generation code example for generating labeled data.  
'supplement/unlabeled_data_generation': This is a data generation code example for generating unlabeled data.

# How to generate extra data
   
1, Use 'supplement/data_generation.py' with model 'Qwen/Qwen2.5-14B-Instruct' and seed 1222 to generate 4000 pieces of data to get gendata_{}.csv.   
2, Use 'supplement/data_generation.py' with model 'mistralai/Mistral-Nemo-Instruct-2407' and seed 1255 to generate 4000 pieces of data to get gendata_{}.csv.  
3, These two pieces of data are combined to create 'data_with_preds10151_10152.csv.'
4, Use supplement/bugged_deberta.py to train a deberta model, where 'data.csv' is the official data, and 'sub_train_data.csv' is 'data_with_preds10151_10152.csv' without soft labels. 
5, Use the trained deberta model to soft label the generated data to get data_with_preds10151_10152.csv with soft labels.
6, Use the trained deberta model to soft label the official training data to get 'data/10202_valpreds.csv'
7, Train a gemma2 model with the official data (combined with the soft labels from 'data/10202_valpreds.csv') and 'data/data_with_preds10151_10152.csv' (one gemma 9b is enough, no need to train multiple models now).   
8, Use 'supplement/unlabeled_data_generation.py' with model 'Qwen/Qwen2.5-7B-Instruct' and seed 11041 to generate 2000 pieces of unlabeled data, and use the model from step 7 to soft label the generated data to get the data of source '11041pure'.
9, Train a gemma2 model with more data from step 8.
10, Use 'supplement/unlabeled_data_generation.py' with model 'Qwen/Qwen2.5-14B-Instruct' and seed 10271 to generate 4000 pieces of unlabeled data, and use the model from step 9 to soft label the generated data to get the data of source '10271puregen'.  
11, Train a gemma2 model with more data from step 10.  
12, Use 'supplement/unlabeled_data_generation.py' with model 'Qwen/Qwen2.5-14B-Instruct' and seed 10291 to generate 4000 pieces of unlabeled data, and use the model from step 11 to soft label the generated data to get the data of source '10291pure'.  
13, Train a gemma2 model with more data from step 12.  
14, Use 'supplement/unlabeled_data_generation.py' with model 'mistralai/Mistral-Nemo-Instruct-2407' and seed 44564 to generate 8000 pieces of unlabeled data, and use the model from step 13 to soft label the generated data to get the data of source '11051pure'.  
15, Manually delete some erroneous data and then combine the generated data to get '11071_padright_p1_alldata_to_1106.csv' and '11071_padright_p2_alldata_to_1106.csv'.
 
